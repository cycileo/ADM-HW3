{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Homework 3](https://github.com/Sapienza-University-Rome/ADM/tree/master/2024/Homework_3) - Michelin restaurants in Italy\n",
    "![iStock-654454404-777x518](https://a.storyblok.com/f/125576/2448x1220/327bb24d32/hero_update_michelin.jpg/m/1224x0/filters:format(webp))\n",
    "\n",
    "## 1. Data collection\n",
    "\n",
    "For the data collection, we wrote the required function in a `data_collection.py` module. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_collection import save_links, download_html_from_link_file, html_to_tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the overview of the main functions for each step, together with the code to run. \n",
    "\n",
    "Every function has an optional `data_folder` argument wich server the purpose to set the working data directory. \n",
    "We tought this to be useful, for example to set the date of the data collection as the directory name. \n",
    "This is useful, as the Michelin list of restaurant is constantly updated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'DATA 11-09'\n",
    "# date of last data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1.1. Get the list of Michelin restaurants\n",
    "   #### **Function**: `save_links`\n",
    "   - **Description**: \n",
    "     Collects restaurant links from the Michelin Guide website starting from the provided `start_url`. The links are saved into a text file (`restaurant_links.txt`) within a specified data folder.\n",
    "   - **Input**: \n",
    "     - `start_url`: URL of the Michelin Guide page to start scraping.\n",
    "   - **Optional Input**: \n",
    "     - `file_name`: name of the output file; by default it is `restaurant_links.txt`.\n",
    "     - `data_folder`: the folder where datas will be stored; by default it is `DATA`.\n",
    "   - **Output**:\n",
    "     - A text file containing restaurant links, one per line, saved in the `data_folder`.\n",
    "   - **Key Features**:\n",
    "     - Automatically detects the number of pages to scrape.\n",
    "     - Skips scraping if the links file already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Links already collected.\n",
      "There are 1982 link already collected\n"
     ]
    }
   ],
   "source": [
    "start_url = \"https://guide.michelin.com/en/it/restaurants\"\n",
    "save_links(start_url, data_folder = data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1.2. Crawl Michelin restaurant pages\n",
    "   #### **Function**: `download_html_from_link_file`\n",
    "   - **Description**: \n",
    "     Downloads the HTML from every URL in the input `file_name`, and saves them to a structured folder (`DATA/HTMLs/page_X`).\n",
    "   - **Input (all optional)**:\n",
    "     - `file_name`: name of the file with the links; by default it is `restaurant_links.txt`.\n",
    "     - `data_folder`: the folder where datas will be stored; by default it is `DATA`.\n",
    "   - **Output**:\n",
    "     - Saves the HTML files in a structured folder `DATA/HTMLs/page_X`. \n",
    "   - **Key Features**:\n",
    "     - Uses `ThreadPoolExecutor` to speed up the process\n",
    "     - Skips existing HTML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download HTMLs: 100%|██████████| 1982/1982 [00:00<00:00, 4185.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All html files have been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "download_html_from_link_file(data_folder = data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1.3 Parse downloaded pages\n",
    "\n",
    "#### **Function**: `extract_info_from_html`\n",
    "- **Description**:  \n",
    "  Parses a restaurant's HTML page and extracts structured information such as name, address, cuisine type, price range, description, and services.\n",
    "- **Input**:\n",
    "  - `html`: The raw HTML content of a restaurant's page.\n",
    "- **Output**:\n",
    "  - A dictionary containing extracted fields.\n",
    "- **Key Features**:\n",
    "  - Handles missing data gracefully.\n",
    "  - Handles addresses separated by commas.\n",
    "\n",
    "\n",
    "#### **Function**: `html_to_tsv`\n",
    "- **Description**:  \n",
    "  Scans the `HTMLs` folder inside the `data_folder` for all the html files, then processes every file with `extract_info_from_html`.\n",
    "- **Input (optional)**:\n",
    "  - `data_folder`: The folder where data will be stored; by default it is `DATA`.\n",
    "  - `max_workers`: the max number of concurrent HTML parsing tasks. \n",
    "- **Output**:\n",
    "  - Saves the TSV files in the folder `DATA/TSVs`.\n",
    "- **Key Features**:\n",
    "     - Uses `ThreadPoolExecutor` to speed up the process. \n",
    "- **Advice**:\n",
    "     - Fine-tune the `max_workers` parameter according to your CPU performance. As a rule of thumb, set `max_workers` to the number of CPU cores available. An estimated processing time of around 5 minutes is typical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing HTMLs: 100%|██████████| 1982/1982 [00:00<00:00, 22717.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files have been processed and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "html_to_tsv(data_folder=data_folder, max_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\leox0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import re\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Text preprocessing setup\n",
    "nltk.download('stopwords')\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
    "    words = text.split()\n",
    "    words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "files: 100%|██████████| 1982/1982 [00:04<00:00, 433.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load all files into a single DataFrame\n",
    "files = glob.glob(data_folder + '/TSVs/restaurant_*.tsv')\n",
    "df_list = []\n",
    "\n",
    "for file in tqdm(files, desc='files'):\n",
    "    restaurant_id = int(file.split('_')[-1].split('.')[0])  # Extract unique ID\n",
    "    data = pd.read_csv(file, sep='\\t')\n",
    "    data['restaurant_id'] = restaurant_id  # Add ID as a new column\n",
    "    df_list.append(data)\n",
    "\n",
    "# Combine all files into one DataFrame\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Apply preprocessing\n",
    "df['processed_description'] = df['description'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vocabulary\n",
    "unique_words = pd.Series([word for words in df['processed_description'] for word in words]).unique()\n",
    "vocab = {word: idx for idx, word in enumerate(unique_words)}\n",
    "\n",
    "# Save Vocabulary\n",
    "pd.DataFrame(list(vocab.items()), columns=['word', 'term_id']).to_csv('vocabulary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Inverted Index\n",
    "inverted_index = {}\n",
    "for i, words in enumerate(df['processed_description']):\n",
    "    restaurant_id = int(df.loc[i, 'restaurant_id'])\n",
    "    for word in words:\n",
    "        term_id = vocab[word]\n",
    "        if term_id not in inverted_index:\n",
    "            inverted_index[term_id] = []\n",
    "        if restaurant_id not in inverted_index[term_id]:\n",
    "            inverted_index[term_id].append(restaurant_id)\n",
    "\n",
    "# Sort each list of document IDs in the inverted index\n",
    "for term_id in inverted_index:\n",
    "    inverted_index[term_id].sort()\n",
    "\n",
    "# Save Inverted Index\n",
    "import json\n",
    "with open('inverted_index.json', 'w') as f:\n",
    "    json.dump(inverted_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
